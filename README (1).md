# mmWalk: Towards Inclusive Multimodal Walking Embodied AI

## ðŸ“¦ Code Submission

This repository contains:

- âœ… Fully annotated QA data, classified by QA type  
- âœ… Inference code for multiple models  
- âœ… Evaluation code using GPT  
- âœ… Sample inference and evaluation results  

---

## ðŸ“ Directory Structure

> âš ï¸ Before running any code, **make sure you have downloaded `QAFrames.zip`** and extracted it into your `Workspace/` directory.

### After extracting the dataset:

```
---Workspace
   |--QAFrames
   |   |--Busstop01
   |   |--...
   |
   |--test_data_chat
   |   |--E1.jsonl
   |   |--...
   |   |--H2.jsonl
   |
   |--inference_lmdeploy.py
   |--inference_transformer.py
   |--eval_gpt.py
```

### After running inference and evaluation:

```
---Workspace
   |--QAFrames
   |   |--Busstop01
   |   |--...
   |
   |--test_data_chat
   |   |--E1.jsonl
   |   |--...
   |   |--H2.jsonl
   |
   |--inference_lmdeploy.py
   |--inference_transformer.py
   |--eval_gpt.py
   |
   |--INFERENCE_OUTPUT_FOLDER
   |   |--E1.jsonl
   |   |--...
   |   |--H2.jsonl
   |
   |--gpt_scored_samples.json
   |--gpt_evaluation_summary.json
```

---

## ðŸ—‚ï¸ Dataset Details

### `test_data_chat/`

This folder contains 9 `.jsonl` files, each corresponding to a QA type category in the test split.

---

## ðŸ“‚ Evaluation Output Format

### `eval_MODELNAME_INPUTSETTING_SHOTSETTING/`

Each folder contains:
- Model inference results
- GPT-evaluated scores and summaries for 9 QA categories

**Naming convention:**

- `MODELNAME`: model used, e.g., `Janus-Pro-7B`, `Qwen2VL-7B-Instruct`
- `INPUTSETTING`:
  - `normal`: full multiview input
  - `single`: walker view only
  - `dog+`: dog + walker views
  - `drone+`: drone + walker views
- `SHOTSETTING`:
  - `0s`: zero-shot
  - `3s`: 3-shot
  - `finetuned`: finetuned model

**Example:**

`eval_Qwen2_normal_3s` means:
> Results from Qwen2-7B-Instruct using full multiview input and 3-shot setting.

ðŸ“ For more details, please refer to our paper.

---

## ðŸš€ Inference Scripts

### `inference_lmdeploy.py`

Model deployment using `lmdeploy`, including our **finetuned model**:

**Model Name:**
```
mmWalkQA_finetuned_internvl2_8b_internlm2_7b_dynamic_res_2nd_merge
```

ðŸ”— Download link: *(To be added)*

**Usage:**

```bash
# Run inference on E1
python inference_lmdeploy.py -E1

# Run inference on 10 samples per category
python inference_lmdeploy.py -testall
```

---

### `inference_transformer.py`

Deployment via HuggingFace `transformers`.  
Usage is identical to `inference_lmdeploy.py`.

ðŸ’¬ Check code comments for details.

---

## ðŸ“Š Evaluation Script

### `eval_gpt.py`

This script runs GPT-based evaluation and produces:

- `gpt_scored_samples.json`: score for each QA pair
- `gpt_evaluation_summary.json`: average score per QA type and scenario

**Note:**
- Average score is formatted with `.2f`, which may introduce Â±1 rounding errors in normalized scoring.

**Usage:**

```bash
python eval_gpt.py
```

ðŸ“Œ The evaluation summary will be printed to the terminal.

---

## ðŸ“¬ Citation

> Coming soon â€” please refer to our paper for citation details.

---

Feel free to open an issue or pull request if you encounter any problems.
